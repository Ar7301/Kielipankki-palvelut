#! /usr/bin/env python3
# -*- mode: Python; -*-

# syncmeta

# This script connects to a metashare node and reads xml
# metadata and meta-metadata into variables for further processing
# Note that the http connection sets cookies which are available
# throughout the lifetime of the script.

# For accessing a metashare node see
# https://github.com/metashare/META-SHARE/wiki/META-SHARE-Harvesting-Protocol-v1.0

# Authors: Martin Matthiesen, CSC; Jussi Piitulainen, HY

# To start from scratch, set up your META-SHARE sync user, set up an
# empty current database in DATA, and update the current database:
# $ sqlite3 DATA/mdfirst.db < schema.sql
# $ (cd DATA; ln -s mdfirst.db current)
# $ ./syncmeta
# There should be a new current database named DATA/mdxxxxxx.db, for
# some xxxxxx.

from http.cookiejar import CookieJar
from urllib.parse import urlencode
from urllib.request import build_opener, HTTPCookieProcessor, Request
from zipfile import ZipFile
from io import BytesIO, TextIOWrapper
from time import time, strftime, gmtime
from tempfile import NamedTemporaryFile as NamedFile
from lxml import etree
import hashlib, json, os.path, shutil, sqlite3
from sys import stderr

dbparent = '/home/u1/jpiitula/foo/Meta/data/'
# dbparent/current -> dbparent/mdxxxxxx.db (before)
# dbparent/current -> dbparent/mdyyyyyy.db (after)

loginURL     = 'http://metashare.csc.fi/login/'
inventoryURL = 'http://metashare.csc.fi/sync?sync_protocol=1.0'
recordURL    = 'http://metashare.csc.fi/sync/{}/metadata/'

syncuser = dict(username = 'metalb_syncuser961',
                password = '!This12Should94Work44As#Password')

# a new oai identifier uses this many characters from the end of the
# storage identifier
stemsize = 4

# protocol, plus "sh" for "share", then that stem
oaiprefix = 'oai:kielipankki.fi:sh'
metasharenode = 'http://metashare.csc.fi'

# There will be an XSL Transform for each metadata prefix
cmditransform = etree.XSLT(etree.parse('xsl/meta-cmdi.xsl')) # partial! now
cmdjtransform = etree.XSLT(etree.parse('xsl/meta-cmdj.xsl')) # the remains.
coretransform = etree.XSLT(etree.parse('xsl/meta-core.xsl'))
infotransform = etree.XSLT(etree.parse('xsl/meta-info.xsl'))
olactransform = etree.XSLT(etree.parse('xsl/meta-olac.xsl'))

# schema-invalid META-SHARE records will be served as "deleted"
infoschema = etree.XMLSchema(etree.parse('xsd/META-XMLSchema/UHEL/'
                                         'META-SHARE-Resource.xsd'))

def now():
    stamp = round(time())
    ztime = strftime('%Y-%m-%dT%H:%M:%SZ', gmtime(stamp))
    return ztime, stamp

def login():
    
    cookieJar = CookieJar()
    opener = build_opener(HTTPCookieProcessor(cookieJar))

    def postdata(**items):
        return urlencode(items).encode('ASCII')

    # Step 1
    # get csrftoken cookie in opener/cookieJar
    # also sessionid in there I think
    print('opening', loginURL, 'with GET')
    opener.open(loginURL)

    # Extract that csrftoken
    token, = ( cookie.value
               for cookie in cookieJar
               if cookie.name == 'csrftoken' )

    # Step 2
    # Login. A new cookie with a new session id is returned.
    print('opening', loginURL, 'with POST')
    opener.open(loginURL,
                postdata(this_is_the_login_form = '1',
                         csrfmiddlewaretoken = token,
                         **syncuser))

    return opener

def fetchinventory(opener):
    # Step 3
    # get inventory. The cookie with the new session id obtained in
    # Step 2 is automatically returned.

    print('opening', inventoryURL)
    response = opener.open(inventoryURL)
    print('inventory:', response.status)
    zipbytes = response.read()
    inventoryStream=ZipFile(BytesIO(zipbytes)).open("inventory.json")
    return json.load(TextIOWrapper(inventoryStream))

def fetch(opener, storid, checksum):
    # Step 4
    # Get the resource data
    # load zipped resource data into zip object
    #print('opening', recordURL, '(with storage identifier)')
    for x in 1, 2, 3:
        try:
            response = opener.open(recordURL.format(storid))
            break
        except Exception as whatever:
            print('attempt', x, whatever)
            # Gives "attempt 1 <urlopen error [Errno -2] Name or
            # service not known>" sometimes, haven't seen more
            # attempts yet.

    if response.status == 200:
        pass
    else:
        print('non-200 record:', response.status, response.reason)
    zipbytes = response.read()
    lrDataZip=ZipFile(BytesIO(zipbytes))

    #unzip xml and storage_global.json
    documentbytes= lrDataZip.read("metadata.xml")
    storagebytes = lrDataZip.read('storage-global.json')
    storage = json.load(TextIOWrapper(BytesIO(storagebytes)))
    
    # compute the hashvalue of the returned data
    md5 = hashlib.md5()
    md5.update(documentbytes)
    md5.update(storagebytes)

    # Debug: is the checksum ok?
    if md5.hexdigest() == checksum:
        pass
    else:
        print("mismatch: %s :: %s" % (md5.hexdigest(), storage))

    return etree.parse(BytesIO(documentbytes)), storage

def dbinventory(cursor):
    cursor.execute('''
            select sourceid, code
            from origin natural join condition
            where sourcename = 'http://metashare.csc.fi'
    ''')
    return dict(cursor)

def dbidentifiers(cursor):
    print('selecting database identifiers')
    cursor.execute('''
            select identifier, sourceid
            from origin
            where sourcename = 'http://metashare.csc.fi'
    ''')
    return dict(cursor)

def synch(connection, opener, oldinv, newinv):
    # newinv is already filtered with seen suffixes
    # Fetch one resource at a time. Check hash values.
    #newinv = fetchinventory(opener)

    cursor = connection.cursor()
    # oldinv = dbinventory(cursor)

    # oldinv : storage identifier -> checksum in database
    # newinv : storage identifier -> checksum in metashare

    pmhids = dbidentifiers(cursor)
    # to ensure uniqueness of new OAI-PMH identifiers

    # previously unknown metashare items
    for stoid in newinv.keys() - oldinv.keys():
        #print('handle creation of', storid)
        doc, sto = fetch(opener, stoid, newinv[stoid])
        create(cursor, doc, sto, newinv[stoid], pmhids)

    # known but changed metashare items
    for stoid in newinv.keys() & oldinv.keys():
        if newinv[stoid] != oldinv[stoid]:
            doc, sto = fetch(opener, stoid, newinv[stoid])
            update(cursor, doc, sto, newinv[stoid])

#    for counter, lrid in enumerate(newinv):
#        
#        doc, sto = fetch(opener, lrid, newinv[lrid])#
#
#        print(#counter, '\t',
#              'status:', sto['publication_status'],
#              'deleted' if sto['deleted'] else 'there',
#              '; modified:', sto['modified'])
#        #print xml
#        
#        if counter > 3:
#            print('breakin')
#            break

    cursor.execute('''
        select min(stamp), max(stamp) from ix
    ''')
    minstamp, maxstamp = cursor.fetchone()
    cursor.execute('''
        update repository
        set minstamp = :minstamp,
            maxstamp = :maxstamp
    ''', dict(minstamp = minstamp, maxstamp = maxstamp))

    connection.commit()
    cursor.close()

def checkdeleted(doc, identifier, sto):
    '''Check the validity of a published META-SHARE record.  For
    unpublished records, return true. For published and invalid
    records, print a report in stderr and return false.'''
    if ( sto['deleted'] or
         sto['publication_status'] != 'p' ):
        return True

    valid = infoschema(doc)
    ( valid
      or print('invalid META-SHARE record:',
               identifier,
               doc.xpath('descendant::info:resourceName/text()',
                         namespaces =
                         dict(info = 'http://www.ilsp.gr/META-XMLSchema')),
               # infoschema.error_log, TOO MUCH
               file = stderr) )
    return not valid

def create(cursor, doc, sto, chk, old):
    # identifier is in sto
    # this identifier is not in current
    # old are the identifiers in current
    # not storage identifiers but pmh, shDDDDDDC
    # must add new identifier to old
    stoid = sto['identifier']
    new = makeidentifier(stoid, old)
    old[new] = stoid
    print('create', new, '=>', stoid)
    cursor.execute('''
            insert into origin(identifier, sourcename, sourceid)
            values (:newid, :source, :stoid)
    ''', dict(newid = new,
              source = metasharenode,
              stoid = stoid))
    cursor.execute('''
            insert into condition(identifier, code)
            values (:newid, :code)
    ''', dict(newid = new, code = chk))

    deleted = checkdeleted(doc, new, sto)
    for model in makeCore, makeOLAC, makeInfo, makeCMDI:
        data = model(new, doc, deleted)
        data['recno'] = makerecno(cursor)
        cursor.execute('''
           insert into record(recno, header, metadata, about)
           values (:recno, :header, :metadata, :about)
        ''', data)
        for spec in {'*'}:
            data['setSpec'] = spec
            cursor.execute('''
               insert into ix(identifier, metadataPrefix, setSpec,
                              stamp, recno)
               values (:identifier, :metadataPrefix, :setSpec,
                       :stamp, :recno)
            ''', data)

def makerecno(cursor):
    # get free recno
    cursor.execute('''
            select coalesce(max(recno), 0) from record
    ''')
    maxrecno, = cursor.fetchone()
    return maxrecno + 1

def makeidentifier(stoid, old):
    stem = stoid[-stemsize:] # last stemsize hex digits
    begin = len(oaiprefix)
    end = begin + stemsize
    new = ( '{}{}{}'
            .format(oaiprefix,
                    stem, sum(1 for x in old
                                if x[begin:end] == stem)) )
    return new

def update(cursor, doc, sto, chk):
    '''Updates the database for an item that has changed. Assume doc
    is deleted (not public) or valid.
    '''
    cursor.execute('''
        select identifier from origin
        where sourcename = :source and sourceid = :stoid
    ''', dict(source = metasharenode,
              stoid = sto['identifier']))
    identifier, = cursor.fetchone()
    cursor.execute('''
        update condition set code = :code
        where identifier = :identifier
    ''', dict(identifier = identifier,
              code = chk))

    deleted = checkdeleted(doc, identifier, sto)
    for model in makeInfo, makeOLAC, makeCore, makeCMDI:
        data = model(identifier, doc, deleted)
        cursor.execute('''
            select recno from ix
            where identifier = :identifier and
                  metadataPrefix = :metadataPrefix and
                  setSpec = '*'
        ''', data)
        recnos = cursor.fetchone()
        if recnos:
            sole, = recnos
            data['recno'] = sole
            print('update', identifier, data['recno'])
            cursor.execute('''
                update record
                set header = :header,
                    metadata = :metadata,
                    about = :about
                where recno = :recno
            ''', data)
            cursor.execute('''
                update ix set stamp = :stamp
                where recno = :recno
            ''', data) # assuming sets remain as they were
        else:
            data['recno'] = makerecno(cursor)
            print('create', identifier, data['recno'])
            cursor.execute('''
                insert into record(recno, header, metadata, about)
                values (:recno, :header, :metadata, :about)
            ''', data)
            print('inserted', data['recno'])
            cursor.execute('''
                insert into ix(xno, identifier,
                               metadataPrefix, setSpec, stamp, recno)
                values(null, :identifier,
                       :metadataPrefix, '*', :stamp, :recno)
            ''', data) # assuming no sets

def makeheader(identifier, ztime, sets, deleted):
    header = ( '<header{deleted}>'
               '<identifier>{identifier}'
               '</identifier>'
               '<datestamp>{ztime}'
               '</datestamp>'
               '</header>' )
    return header.format(deleted = ( ' status="deleted"'
                                     if deleted else '' ),
                         identifier = identifier,
                         ztime = ztime)

def makeCore(new, doc, deleted):
    '''Returns the components of a Dublin Core record as a dict to use
    in a database insertion statement for record or ix.'''
    if deleted:
        metadata = ''
    else:
        metadata = coretransform(doc)
        metadata = etree.tostring(metadata, encoding=str)
    ztime, stamp = now()
    header = makeheader(new, ztime, (), deleted)
    return dict(identifier = new,
                metadataPrefix = 'oai_dc',
                stamp = stamp,
                header = header,
                metadata = metadata,
                about = '')

def makeOLAC(new, doc, deleted):
    '''Returns the components of an OLAC record as a dict to use in a
    database insertion statement for record or ix.'''
    if deleted:
        metadata = ''
    else:
        metadata = olactransform(doc)
        metadata = etree.tostring(metadata, encoding=str)
    ztime, stamp = now()
    header = makeheader(new, ztime, (), deleted)
    return dict(identifier = new,
                metadataPrefix = 'olac',
                stamp = stamp,
                header = header,
                metadata = metadata,
                about = '')

def makeInfo(new, doc, deleted):
    '''Returns the components of a META-SHARE ResourceInfo record as a
    dict to use in a database insertion statement for record or ix.'''
    if deleted:
        metadata = ''
    else:
        metadata = infotransform(doc)
        metadata = etree.tostring(metadata, encoding=str)
    ztime, stamp = now()
    header = makeheader(new, ztime, (), deleted)
    return dict(identifier = new,
                metadataPrefix = 'info',
                stamp = stamp,
                header = header,
                metadata = metadata,
                about = '')

def makeCMDI(new, doc, deleted):
    '''Returns the components of a CMDI record as a dict to use in a
    database insertion statement for record or ix.'''
    cmd = cmdjtransform(cmditransform(doc)) # from Athens
    xxx, = cmd.xpath('attribute::xsi:schemaLocation',
                     namespaces =
                     dict(xsi = 'http://www.w3.org/2001/XMLSchema-instance'))
    print('schema location:', xxx)
    # ...clarin.eu:cr1:p_1361876010571/xsd
    prefix = 'cmdi{}'.format(xxx[-8:-4])
    metadata = ( ''
                 if deleted
                 else etree.tostring(cmd, encoding=str) )
    ztime, stamp = now()
    header = makeheader(new, ztime, (), deleted)
    return dict(identifier = new,
                metadataPrefix = prefix,
                stamp = stamp,
                header = header,
                metadata = metadata,
                about = '')

def main(see = ['']):
    current = os.path.join(dbparent, 'current')
    opener = login()
    newinv = fetchinventory(opener)

    connection = sqlite3.connect(current)
    cursor = connection.cursor()
    oldinv = dbinventory(cursor)
    connection.close()

    # checksums indicate that nothing changed --
    # now this ignores those that have vanished <-- BOTHER
    # (we are not likely to have those for quite a while -
    # - something has gone wrong in META-SHARE side if that
    # happens)
    if all(newinv[stoid] == oldinv.get(stoid, '--')
           for stoid in newinv
           if any(stoid.endswith(six)
                  for six in see)):
        print('database is up-to-date modulo', see)
        exit(0)

    newdb = NamedFile(dir = dbparent, prefix = 'md', suffix = '.db',
                      delete = False)
    newdb.close()
    # shutil.copy2 calls copystat and raises this exception on hippu:
    # "OSError: [Errno 95] Operation not supported"
    shutil.copy(current, newdb.name)
    synch(sqlite3.connect(newdb.name), opener, oldinv,
          { stoid : newinv[stoid]
            for stoid in newinv
            if any(stoid.endswith(six)
                   for six in see) })
    # set current -> newdb.name
    os.remove(current) # race condition!
    os.symlink(newdb.name, current)

if __name__ == '__main__':
    from optparse import OptionParser
    parser = OptionParser()
    parser.add_option('--see', action = 'store', default = '', # see all
                      help = 'suffixes of seen storage ids, comma-separated')
    opts, args = parser.parse_args()
    main(opts.see.split(','))
